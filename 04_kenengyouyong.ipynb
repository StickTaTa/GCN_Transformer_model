{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class CompoundProteinInteractionPrediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompoundProteinInteractionPrediction, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(n_fingerprint, dim)\n",
    "        self.embed_word = nn.Embedding(n_word, dim)\n",
    "\n",
    "        # 定义每一层的图神经网络结构,全连接层，输入输出维度相同都是dim，并且创建layer_gnn个全连接层\n",
    "        self.W_GNN = nn.ModuleList([nn.Linear(dim, dim) for _ in range(layer_gnn)])\n",
    "\n",
    "        # 定义长短时循环神经网络，这里我想替换成transformer，但是方法还不成熟，之后试试\n",
    "        # 输入的参数依次是：词向量的维度，隐藏层的维度，堆叠的层数\n",
    "        self.Bi_LSTM = nn.LSTM(dim, 5, 1, dropout=0.2, bidirectional=True)\n",
    "\n",
    "        # 添加注意力机制，这部分参数来自图神经网络\n",
    "        self.W_attention = nn.Linear(dim, dim)\n",
    "\n",
    "        # 最终输出的结果\n",
    "        self.W_out = nn.ModuleList([nn.Linear(2 * dim, 2 * dim) for _ in range(layer_output)])\n",
    "\n",
    "        # 最最终的结果\n",
    "        self.W_interaction = nn.Linear(2 * dim, 2)\n",
    "\n",
    "    def gnn(self, fingerprint_vector, adjacency, layer_num):\n",
    "        # 这里的layer_num只是作为一个数字，进行循环使用的，使用的数字就是上面函数中定义的layer_gnn的数量\n",
    "        for i in range(layer_num):\n",
    "            index = torch.relu(self.W_GNN[i](fingerprint_vector))\n",
    "            fingerprint_vector = fingerprint_vector + torch.matmul(adjacency, index)\n",
    "        return torch.unsqueeze(torch.mean(fingerprint_vector, 0), 0)\n",
    "\n",
    "    def attention_cnn(self, compound_vector, word_vectors, layer_num):\n",
    "        # 这里的layer_num的意义同上，这个也是循环次数，循环次数是layer_cnn的层数\n",
    "        word_vectors = torch.unsqueeze(word_vectors, 0)\n",
    "\n",
    "        # 作者的源代码就是直接进行了BiLSTM，并没有进行前面的Bert训练\n",
    "        bilstm, _ = self.Bi_LSTM(word_vectors)\n",
    "        bilstm = torch.squeeze(bilstm, 0)\n",
    "\n",
    "        h = torch.relu(self.W_attention(compound_vector))\n",
    "        hs = torch.relu(self.W_attention(bilstm))\n",
    "        weights = torch.tanh(F.linear(h, hs))\n",
    "\n",
    "        ys = torch.t(weights) * hs\n",
    "\n",
    "        return torch.unsqueeze(torch.mean(ys, 0), 0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        fingerprints, adjacency, word = inputs\n",
    "\n",
    "        \"\"\"药物分子处理\"\"\"\n",
    "        fingerprint_vector = self.embed_fingerprint(fingerprints)\n",
    "        compound_vector = self.gnn(fingerprint_vector, adjacency, layer_gnn)\n",
    "\n",
    "        \"\"\"蛋白序列处理\"\"\"\n",
    "        word_vectors = self.embed_word(word)\n",
    "        protein_vector = self.attention_cnn(compound_vector, word_vectors, layer_cnn)\n",
    "\n",
    "        \"\"\"将向量结合并输出交互结果\"\"\"\n",
    "        cat_vector = torch.cat((compound_vector, protein_vector), 1)\n",
    "        # layer_out目的和之前相同，仅作为数字进行循环使用\n",
    "        for j in range(layer_output):\n",
    "            cat_vector = torch.relu(self.W_out[j](cat_vector))\n",
    "        interaction = self.W_interaction(cat_vector)\n",
    "        return interaction\n",
    "\n",
    "    def __call__(self, data, train=True):\n",
    "        inputs, correct_interaction = data[:-1], data[-1]\n",
    "        prediction_interaction = self.forward(inputs)\n",
    "\n",
    "        if train:\n",
    "            loss = F.cross_entropy(prediction_interaction, correct_interaction)\n",
    "            return loss\n",
    "        else:\n",
    "            correct_labels = correct_interaction.to('cpu').data.numpy()\n",
    "            ys = F.softmax(prediction_interaction, 1).to('cpu').data.numpy()\n",
    "            predicted_labels = list(map(lambda x: np.argmax(x), ys))\n",
    "            predicted_scores = list(map(lambda x: x[1], ys))\n",
    "            return correct_labels, predicted_labels, predicted_scores\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def train(self, dataset):\n",
    "        np.random.shuffle(dataset)\n",
    "        N = len(dataset)\n",
    "        loss_total = 0\n",
    "        for data in dataset:\n",
    "            loss = self.model(data)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.to('cpu').data.numpy()\n",
    "        return loss_total\n",
    "\n",
    "\n",
    "class Tester(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def test(self, dataset):\n",
    "        N = len(dataset)\n",
    "        T, Y, S = [], [], []\n",
    "        for data in dataset:\n",
    "            (correct_labels, predicted_labels, predicted_scores) = self.model(data, train=False)\n",
    "            T.append(correct_labels)\n",
    "            Y.append(predicted_labels)\n",
    "            S.append(predicted_scores)\n",
    "        AUC = roc_auc_score(T, S)\n",
    "        precision = precision_score(T, Y)\n",
    "        recall = recall_score(T, Y)\n",
    "        return AUC, precision, recall\n",
    "\n",
    "    def save_AUCs(self, AUCs, filename):\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write('\\t'.join(map(str, AUCs)) + '\\n')\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def load_tensor(file_name, dtype):\n",
    "    return [dtype(d).to(device) for d in np.load(file_name + '.npy', allow_pickle=True)]\n",
    "\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def shuffle_dataset(dataset, seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, ratio):\n",
    "    n = int(ratio * len(dataset))\n",
    "    dataset_1, dataset_2 = dataset[:n], dataset[n:]\n",
    "    return dataset_1, dataset_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code uses GPU...\n",
      "Training...(BiLSTM)\n",
      "Epoch\tTime(sec)\tLoss_train\tAUC_dev\tAUC_test\tPrecision_test\tRecall_test\n",
      "1\t24.063715900000034\t2183.6258826682606\t0.9381303174932558\t0.9406742001015744\t0.9169139465875371\t0.8631284916201117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17892\\874249999.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     66\u001B[0m             \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparam_groups\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'lr'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*=\u001B[0m \u001B[0mlr_decay\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 68\u001B[1;33m         \u001B[0mloss_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     69\u001B[0m         \u001B[0mAUC_dev\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtester\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset_dev\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m         \u001B[0mAUC_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprecision_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecall_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtester\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17892\\86264053.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m     90\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     91\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 92\u001B[1;33m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     93\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m             \u001B[0mloss_total\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'cpu'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    361\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    362\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 363\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    364\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    365\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    173\u001B[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0;32m    174\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 175\u001B[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[0;32m    176\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    177\u001B[0m def grad(\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"Hyperparameters.\"\"\"\n",
    "    (DATASET, radius, ngram, dim, layer_gnn, window, layer_cnn, layer_output,\n",
    "     lr, lr_decay, decay_interval, weight_decay, iteration,\n",
    "     setting) = ['human', 2, 3, 10, 3, 11, 3, 3, 1e-3, 0.5, 10, 1e-6, 100,\n",
    "                 'human--radius2--ngram3--dim10--layer_gnn3--window11--layer_cnn3--layer_output3--lr1e-3--lr_decay0.5--decay_interval10--weight_decay1e-6--iteration100']\n",
    "    (dim, layer_gnn, window, layer_cnn, layer_output, decay_interval,\n",
    "     iteration) = map(int, [dim, layer_gnn, window, layer_cnn, layer_output,\n",
    "                            decay_interval, iteration])\n",
    "    lr, lr_decay, weight_decay = map(float, [lr, lr_decay, weight_decay])\n",
    "\n",
    "    \"\"\"CPU or GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print('The code uses GPU...')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print('The code uses CPU!!!')\n",
    "\n",
    "    \"\"\"Load preprocessed data.\"\"\"\n",
    "    #    dir_input = ('../dataset/' + DATASET + '/input/'\n",
    "    #                 'radius' + str(radius) + '_ngram' + str(ngram) + '/')\n",
    "    # dir_input = ('../dataset/' + DATASET + '/input/for_train13/')\n",
    "    dir_input = os.getcwd() + '\\\\dataset\\\\' + DATASET\n",
    "    compounds = load_tensor(dir_input + '\\\\compounds', torch.LongTensor)\n",
    "    adjacencies = load_tensor(dir_input + '\\\\adjacencies', torch.FloatTensor)\n",
    "    proteins = load_tensor(dir_input + '\\\\proteins', torch.LongTensor)\n",
    "    interactions = load_tensor(dir_input + '\\\\interactions', torch.LongTensor)\n",
    "    fingerprint_dict = load_pickle(dir_input + '\\\\fingerprint_dict.pickle')\n",
    "    word_dict = load_pickle(dir_input + '\\\\word_dict.pickle')\n",
    "    n_fingerprint = len(fingerprint_dict)\n",
    "    n_word = len(word_dict)\n",
    "\n",
    "    \"\"\"Create a dataset and split it into train/dev/test.\"\"\"\n",
    "    dataset = list(zip(compounds, adjacencies, proteins, interactions))\n",
    "    dataset = shuffle_dataset(dataset, 1234)\n",
    "    dataset_train, dataset_ = split_dataset(dataset, 0.8)\n",
    "    dataset_dev, dataset_test = split_dataset(dataset_, 0.5)\n",
    "\n",
    "    \"\"\"Set a model.\"\"\"\n",
    "    torch.manual_seed(1234)\n",
    "    model = CompoundProteinInteractionPrediction().to(device)\n",
    "    trainer = Trainer(model)\n",
    "    tester = Tester(model)\n",
    "\n",
    "    \"\"\"Output files.\"\"\"\n",
    "    output_path = os.getcwd() + '\\\\new_model_output\\\\'\n",
    "    if not os.path.exists(output_path + 'human\\\\'):\n",
    "        os.mkdir(output_path + 'human\\\\')\n",
    "    file_AUCs = output_path + DATASET + '\\\\AUCs--' + setting + '.txt'\n",
    "    file_model = output_path + DATASET + '\\\\' + setting\n",
    "    AUCs = ('Epoch\\tTime(sec)\\tLoss_train\\tAUC_dev\\t'\n",
    "            'AUC_test\\tPrecision_test\\tRecall_test')\n",
    "    with open(file_AUCs, 'w') as f:\n",
    "        f.write(AUCs + '\\n')\n",
    "\n",
    "    \"\"\"Start training.\"\"\"\n",
    "    print('Training...(BiLSTM)')\n",
    "    print(AUCs)\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    for epoch in range(1, iteration):\n",
    "\n",
    "        if epoch % decay_interval == 0:\n",
    "            trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "\n",
    "        loss_train = trainer.train(dataset_train)\n",
    "        AUC_dev = tester.test(dataset_dev)[0]\n",
    "        AUC_test, precision_test, recall_test = tester.test(dataset_test)\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        time = end - start\n",
    "\n",
    "        AUCs = [epoch, time, loss_train, AUC_dev,\n",
    "                AUC_test, precision_test, recall_test]\n",
    "        tester.save_AUCs(AUCs, file_AUCs)\n",
    "        tester.save_model(model, file_model)\n",
    "\n",
    "        print('\\t'.join(map(str, AUCs)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}